{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40386,"status":"ok","timestamp":1765226161488,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"eFdftzySj6SR","outputId":"d2e2fa07-fbc4-4258-bcbf-49629b1600f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks\n"]}],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Change directory to the folder containing your notebook or dataset\n","%cd /content/drive/My\\ Drive/Colab\\ Notebooks/"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4072,"status":"ok","timestamp":1765226165570,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"c4sqX1b2kkL3"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Load files\n","rfm = pd.read_csv(\"rfm.xls\")\n","context_recon = pd.read_csv(\"contextual_policy_recommendations.xls\")\n","context_policy = pd.read_csv(\"contextual_policy_summary.xls\")\n","context_action = pd.read_csv(\"contextual_policy_tier_action_mix.xls\")\n","retail = pd.read_csv(\"Online_retail_cleaned.xls\")"]},{"cell_type":"markdown","metadata":{"id":"i44mUT1Mg5YL"},"source":["Google API key Installation"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3295,"status":"ok","timestamp":1765226168874,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"wJCcehw6kkJS","outputId":"ac6d0666-da57-457a-ecb4-31faac626536"},"outputs":[{"name":"stdout","output_type":"stream","text":["Please enter your Google API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n","API key successfully loaded for this session.\n"]}],"source":["import os\n","from dotenv import load_dotenv\n","\n","# Try to load the API key from a .env file.\n","load_dotenv()\n","api_key = os.getenv(\"GOOGLE_API_KEY\")\n","\n","# If the key is not found, prompt the user to enter it.\n","if not api_key:\n","    from getpass import getpass\n","    api_key = getpass(\"Please enter your Google API key: \")\n","    os.environ['GOOGLE_API_KEY'] = api_key\n","\n","    # Check if the key was entered\n","    if not api_key:\n","        raise ValueError(\"API key not entered. Please provide your key.\")\n","\n","# You can now proceed with initializing the client\n","# The `api_key` variable is now guaranteed to exist for this session.\n","print(\"API key successfully loaded for this session.\")\n"]},{"cell_type":"markdown","metadata":{"id":"XiP3gx9Ng8wY"},"source":["**Generating Natural Language Customer RFM Summaries**"]},{"cell_type":"markdown","metadata":{"id":"REmhYNBlhBOf"},"source":["This code snippet iterates through a pandas DataFrame named rfm, which presumably contains the results of a Recency, Frequency, and Monetary (RFM) analysis. For every customer record, it extracts their RFM metrics and their churn status (from the Churn_Label column). It then uses an f-string to generate a descriptive, human-readable text summary of the customer's behavior and classification. These summaries are collected into a list called rfm_docs, effectively converting structured numerical data into a list of natural language documents, before printing the first five for review."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1765226169115,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"p0pq2ZQikkGp","outputId":"b28ebafd-abba-4781-ba33-ef90bc49990e"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Customer 12346 has a recency of 66 days, a purchase frequency of 14 times, and a total monetary spend of $372.86. This customer is classified as churned.', 'Customer 12347 has a recency of 2 days, a purchase frequency of 2 times, and a total monetary spend of $1323.32. This customer is classified as not churned.', 'Customer 12348 has a recency of 73 days, a purchase frequency of 1 times, and a total monetary spend of $222.16. This customer is classified as churned.', 'Customer 12349 has a recency of 42 days, a purchase frequency of 3 times, and a total monetary spend of $2064.39. This customer is classified as churned.', 'Customer 12351 has a recency of 10 days, a purchase frequency of 1 times, and a total monetary spend of $300.93. This customer is classified as not churned.']\n"]}],"source":["import pandas as pd\n","\n","rfm_docs = []\n","for index, row in rfm.iterrows():\n","    customer_id = row['Customer ID']\n","    recency = row['Recency']\n","    frequency = row['Frequency']\n","    monetary = row['Monetary']\n","    churn = 'churned' if row['Churn_Label'] == 1 else 'not churned'\n","\n","    text = f\"Customer {customer_id} has a recency of {recency} days, a purchase frequency of {frequency} times, and a total monetary spend of ${monetary:.2f}. This customer is classified as {churn}.\"\n","    rfm_docs.append(text)\n","\n","# You can now see the first few documents\n","print(rfm_docs[:5])"]},{"cell_type":"markdown","metadata":{"id":"s6ALzjvHhEg2"},"source":["**Generating Contextual Policy Recommendation Summaries**"]},{"cell_type":"markdown","metadata":{"id":"Ik5__2evhEZB"},"source":["This code snippet processes a DataFrame named context_recon (which contains contextual policy recommendations). It iterates over each customer record to extract the Customer_ID, the Chosen_Action (the recommended retention strategy), and the Estimated_Reward (the projected financial return, or ROI) for that action. It then converts this structured data into a descriptive text format, creating a sentence that summarizes the recommended action and its predicted ROI for each specific customer. Finally, these text summaries are collected in a list called policy_docs, with the first five documents being printed."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1765226169348,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"AST0ByHhkkEP","outputId":"bae79d32-70af-40ea-f8e6-42d057545dc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[\"For customer 12346, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $2.15.\", \"For customer 12347, the recommended retention action is to use 'email'. This action has a projected ROI of $0.00.\", \"For customer 12348, the recommended retention action is to use 'none'. This action has a projected ROI of $0.00.\", \"For customer 12349, the recommended retention action is to use 'sms'. This action has a projected ROI of $2.99.\", \"For customer 12351, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $0.00.\"]\n"]}],"source":["import pandas as pd\n","\n","# Assuming you've already loaded the dataframe\n","# context_recon = pd.read_csv(\"contextual_policy_recommendations.csv\")\n","\n","policy_docs = []\n","for index, row in context_recon.iterrows():\n","    customer_id = row['Customer_ID']\n","    action = row['Chosen_Action']\n","    reward = row['Estimated_Reward']\n","\n","    text = f\"For customer {customer_id}, the recommended retention action is to use '{action}'. This action has a projected ROI of ${reward:.2f}.\"\n","    policy_docs.append(text)\n","\n","print(policy_docs[:5])"]},{"cell_type":"markdown","metadata":{"id":"0GqVDbbOhM7U"},"source":["**Generating Contextual Policy Action Summaries**"]},{"cell_type":"markdown","metadata":{"id":"IS2ql3W2hQ5d"},"source":["This code snippet aggregates a summary of recommended actions from a DataFrame named context_policy. It iterates through each row, extracting the specific retention action (Chosen_Action), the average projected return on investment (Average_Reward), and the volume (number of customers) assigned to that action. The code then compiles this information into a list of descriptive text documents (summary_docs), which clearly state how many customers were given a particular action and what the action's overall average projected financial benefit is, before printing the resulting summary documents."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1765226169355,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"EmBuquWYkkBq","outputId":"f5b58852-fd0d-4e91-d3e8-4927f334e067"},"outputs":[{"name":"stdout","output_type":"stream","text":["[\"The retention action 'call+coupon' was assigned to 2247 customers, with an average projected ROI of $3.74.\", \"The retention action 'sms+coupon' was assigned to 803 customers, with an average projected ROI of $2.13.\", \"The retention action 'sms' was assigned to 359 customers, with an average projected ROI of $0.05.\", \"The retention action 'email' was assigned to 462 customers, with an average projected ROI of $0.00.\", \"The retention action 'none' was assigned to 418 customers, with an average projected ROI of $0.00.\"]\n"]}],"source":["import pandas as pd\n","\n","# Assuming you've already loaded the dataframe\n","# context_policy = pd.read_csv(\"contextual_policy_summary.csv\")\n","\n","summary_docs = []\n","for index, row in context_policy.iterrows():\n","    action = row['Chosen_Action']\n","    avg_reward = row['Average_Reward']\n","    volume = row['Customers']\n","\n","    text = f\"The retention action '{action}' was assigned to {volume} customers, with an average projected ROI of ${avg_reward:.2f}.\"\n","    summary_docs.append(text)\n","\n","print(summary_docs)"]},{"cell_type":"markdown","metadata":{"id":"5tLfgkGDhUGN"},"source":["**Extracting All Textual Content**"]},{"cell_type":"markdown","metadata":{"id":"uuqoDbuMhXv1"},"source":["This code snippet's purpose is to read and extract all the textual contentâ€”both markdown and codeâ€”from a Jupyter Notebook file. It first uses the json library to load the raw structure of the notebook specified by notebook_path. It then iterates through every cell in the notebook, checks if the cell contains source code or text (in the source field), joins all the lines from that cell, and concatenates them into a single, continuous string named notebook_text. This effectively flattens the notebook's content into a searchable text document, with the final line printing the first 500 characters to verify the extraction."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1765226169763,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"Uksp5B1akj-G","outputId":"535f77bd-d76e-4558-9298-9b71a4f0c44f"},"outputs":[{"name":"stdout","output_type":"stream","text":["**Loading Dataset \u0026 Info**\n","\n","This Python code snippet defines a function, process_online_retail_data, that downloads, cleans, and transforms the \"Online Retail\" dataset from a UCI Machine Learning repository URL. It first uses the requests library to fetch the Excel file and pandas to load it into a DataFrame. The function then performs extensive data cleaningâ€”including dropping missing CustomerID values, removing duplicates, filtering out non-positive unit prices, and handling outliers based on \n"]}],"source":["import json\n","\n","notebook_path = \"E-commerce_1_1.ipynb\"\n","with open(notebook_path, 'r', encoding='utf-8') as f:\n","    notebook_content = json.load(f)\n","\n","# Concatenate all text and code cells into a single string\n","notebook_text = \"\"\n","for cell in notebook_content['cells']:\n","    if 'source' in cell and isinstance(cell['source'], list):\n","        notebook_text += \"\".join(cell['source']) + \"\\n\\n\"\n","\n","# You can now print the first 500 characters to verify\n","print(notebook_text[:500])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1765226169771,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"RIZHjnn2kj7V","outputId":"6ce131f6-3993-4ed6-80c1-2613cbe5e7b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total documents in your knowledge base: 8584\n"]}],"source":["# rfm_docs, policy_docs, summary_docs\n","\n","all_docs = rfm_docs + policy_docs + summary_docs + [notebook_text]\n","\n","print(f\"Total documents in your knowledge base: {len(all_docs)}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":989},"executionInfo":{"elapsed":5469,"status":"ok","timestamp":1765226175238,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"OJi1iJ4Ikj4Z","outputId":"adb2e3b4-59dc-481f-c059-d2f9672eb815"},"outputs":[{"name":"stdout","output_type":"stream","text":["models/embedding-gecko-001\n","models/gemini-2.5-flash\n","models/gemini-2.5-pro\n","models/gemini-2.0-flash-exp\n","models/gemini-2.0-flash\n","models/gemini-2.0-flash-001\n","models/gemini-2.0-flash-exp-image-generation\n","models/gemini-2.0-flash-lite-001\n","models/gemini-2.0-flash-lite\n","models/gemini-2.0-flash-lite-preview-02-05\n","models/gemini-2.0-flash-lite-preview\n","models/gemini-2.0-pro-exp\n","models/gemini-2.0-pro-exp-02-05\n","models/gemini-exp-1206\n","models/gemini-2.5-flash-preview-tts\n","models/gemini-2.5-pro-preview-tts\n","models/gemma-3-1b-it\n","models/gemma-3-4b-it\n","models/gemma-3-12b-it\n","models/gemma-3-27b-it\n","models/gemma-3n-e4b-it\n","models/gemma-3n-e2b-it\n","models/gemini-flash-latest\n","models/gemini-flash-lite-latest\n","models/gemini-pro-latest\n","models/gemini-2.5-flash-lite\n","models/gemini-2.5-flash-image-preview\n","models/gemini-2.5-flash-image\n","models/gemini-2.5-flash-preview-09-2025\n","models/gemini-2.5-flash-lite-preview-09-2025\n","models/gemini-3-pro-preview\n","models/gemini-3-pro-image-preview\n","models/nano-banana-pro-preview\n","models/gemini-robotics-er-1.5-preview\n","models/gemini-2.5-computer-use-preview-10-2025\n","models/embedding-001\n","models/text-embedding-004\n","models/gemini-embedding-exp-03-07\n","models/gemini-embedding-exp\n","models/gemini-embedding-001\n","models/aqa\n","models/imagen-4.0-generate-preview-06-06\n","models/imagen-4.0-ultra-generate-preview-06-06\n","models/imagen-4.0-generate-001\n","models/imagen-4.0-ultra-generate-001\n","models/imagen-4.0-fast-generate-001\n","models/veo-2.0-generate-001\n","models/veo-3.0-generate-001\n","models/veo-3.0-fast-generate-001\n","models/veo-3.1-generate-preview\n","models/veo-3.1-fast-generate-preview\n","models/gemini-2.0-flash-live-001\n","models/gemini-live-2.5-flash-preview\n","models/gemini-2.5-flash-live-preview\n","models/gemini-2.5-flash-native-audio-latest\n","models/gemini-2.5-flash-native-audio-preview-09-2025\n"]}],"source":["import google.generativeai as genai\n","import os\n","\n","# Ensure your API key is configured\n","GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n","genai.configure(api_key=GOOGLE_API_KEY)\n","\n","# List all available models and print their names\n","for model in genai.list_models():\n","    print(model.name)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29606,"status":"ok","timestamp":1765226204853,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"bDUXMHSQkj1f","outputId":"3f52e1f1-44cf-44b6-f191-ce4c4621914b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n","google-adk 1.20.0 requires opentelemetry-api\u003c=1.37.0,\u003e=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n","google-adk 1.20.0 requires opentelemetry-sdk\u003c=1.37.0,\u003e=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n","opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk\u003c1.39.0,\u003e=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# Install necessary libraries for RAG components\n","!pip install -qU chromadb langchain-text-splitters"]},{"cell_type":"markdown","metadata":{"id":"-03qBU7whwqL"},"source":["**Structuring Documents for Vector Indexing (RAG Preparation)**"]},{"cell_type":"markdown","metadata":{"id":"petiOhg3hykB"},"source":["This code snippet is performing a crucial data preparation step for a Retrieval-Augmented Generation (RAG) system by intelligently structuring documents for vector indexing. It first separates a single, long document (notebook_text, which is the entire content of the Colab notebook) from a list of shorter, specific summary documents (short_docs). It then uses the RecursiveCharacterTextSplitter to break the long notebook text into smaller, overlapping chunks (1000 characters with 200 character overlap) to ensure comprehensive context is preserved across splits. Finally, it combines these newly chunked notebook sections with the original short summary documents (like the RFM and policy recommendation texts) into a unified list called final_documents, ready to be indexed for efficient retrieval by a conversational AI."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41983,"status":"ok","timestamp":1765226246838,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"t4fgkvjXkjpq","outputId":"543ce706-b5a6-4ce6-b638-bac6dd563e6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total documents after chunking: 8724\n","Example chunk: **The contextual bandit (LinUCB) model demonstrates a critical bias towards the most expensive action, call+coupon, recommending it for virtually $99\\...\n"]}],"source":["# Preprocessing and Chunking\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","# Separate the long notebook text from the rest of the documents\n","notebook_text = all_docs[-1]\n","short_docs = all_docs[:-1]\n","\n","# 1. Chunk the long notebook document\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=200,\n","    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",")\n","chunked_notebook_docs = text_splitter.create_documents([notebook_text])\n","\n","# Convert LangChain Document objects back to simple strings\n","chunked_text_list = [doc.page_content for doc in chunked_notebook_docs]\n","\n","# 2. Combine all documents: short, specific documents + chunked notebook text\n","final_documents = short_docs + chunked_text_list\n","\n","print(f\"Total documents after chunking: {len(final_documents)}\")\n","print(f\"Example chunk: {final_documents[-1][:150]}...\")"]},{"cell_type":"markdown","metadata":{"id":"y14RZDj-mHfR"},"source":["**Vectorizing and Indexing Knowledge Base (RAG System Setup)**"]},{"cell_type":"markdown","metadata":{"id":"uQlCyFrJiDBy"},"source":["This code snippet performs the critical task of vectorizing and indexing a list of documents to create a knowledge base for a Retrieval-Augmented Generation (RAG) system. It initializes the Google Gemini client and defines the gemini-embedding-001 model for vector creation. It then sets up a persistent ChromaDB vector store in a local directory. The code proceeds to iterate through the final_documents (the combined, chunked text) in batches. For each batch, it calls the Gemini API to generate high-quality vector embeddings optimized for document retrieval. Finally, it stores these embeddings, along with their original text content and unique IDs, into the ChromaDB collection, thereby making the knowledge base searchable via semantic similarity."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":392457,"status":"ok","timestamp":1765226639294,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"9_Jca1K8mJRr","outputId":"11b9bb98-ee86-4f1d-c128-68b4179b2554"},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully indexed 8763 documents.\n","Vector Store is ready for Retrieval.\n"]}],"source":["# Creating and Indexing Embeddings\n","import google.generativeai as genai\n","import chromadb\n","import os\n","\n","# Initialize the Gemini Client (already configured from cell 9)\n","# Ensure os.getenv('GOOGLE_API_KEY') is available.\n","\n","# 1. Define the embedding model\n","EMBEDDING_MODEL = 'models/gemini-embedding-001'\n","\n","# 2. Setup the ChromaDB Client and Collection\n","# This creates a directory 'chroma_db_ecommerce' to store the vector database\n","client = chromadb.PersistentClient(path=\"./chroma_db_ecommerce\")\n","collection = client.get_or_create_collection(\n","    name=\"ecommerce_rag_knowledge_base\",\n",")\n","\n","# 3. Create Embeddings in Batches and Store in ChromaDB\n","# For performance, we generate and store embeddings in small batches.\n","batch_size = 100\n","for i in range(0, len(final_documents), batch_size):\n","    batch_docs = final_documents[i:i + batch_size]\n","    batch_ids = [f\"doc_{j}\" for j in range(i, i + len(batch_docs))]\n","\n","    # Generate embeddings using the Gemini API\n","    # We use a list comprehension to handle the response structure\n","    result = genai.embed_content(\n","        model=EMBEDDING_MODEL,\n","        content=batch_docs,\n","        task_type=\"RETRIEVAL_DOCUMENT\" # Optimizes embeddings for RAG retrieval\n","    )\n","    batch_embeddings = result['embedding']\n","\n","    # Add the embeddings and text to ChromaDB\n","    collection.add(\n","        embeddings=batch_embeddings,\n","        documents=batch_docs,\n","        ids=batch_ids\n","    )\n","\n","print(f\"Successfully indexed {collection.count()} documents.\")\n","print(\"Vector Store is ready for Retrieval.\")"]},{"cell_type":"markdown","metadata":{"id":"yXUOFOWAoAhD"},"source":["**Implementing the Core RAG Chatbot Query Function**"]},{"cell_type":"markdown","metadata":{"id":"33C0l8N7iPe7"},"source":["This code snippet defines and demonstrates the core function of a Retrieval-Augmented Generation (RAG) chatbot designed for E-commerce analytics. The rag_chat_query function takes a user question and performs a two-step process: Retrieval and Generation.\n","\n","Retrieval: It first converts the user's question into a vector embedding using the Gemini API. It then uses this vector to query the pre-built ChromaDB vector store for the top_k most semantically relevant text chunks from the knowledge base, which become the context.\n","\n","Generation: It augments a prompt by combining the retrieved context with the original user question, and applies a system instruction to act as an expert E-commerce chatbot. Finally, it passes this complete prompt to the Gemini-2.5-flash model, which generates a concise answer based only on the provided context, thus ensuring grounded and factual responses."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277},"executionInfo":{"elapsed":1704,"status":"ok","timestamp":1765226641001,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"G7JEXMMcmOCF","outputId":"89085e57-1d62-4bf0-a6f7-27aea1dd1682"},"outputs":[{"name":"stdout","output_type":"stream","text":["User Question: What is the recommended retention action and projected ROI for customer 12349, and what is their recency?\n","\n","--- Chatbot Answer ---\n","For customer 12349, the recommended retention action is 'sms', with a projected ROI of $2.99. Information regarding their recency is not available in the provided context.\n","\n","--- Context Used for Answer ---\n","For customer 12349, the recommended retention action is to use 'sms'. This action has a projected ROI of $2.99.\n","---\n","For customer 12549, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $7.56.\n","---\n","For customer 17349, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $3.18.\n","---\n","For customer 13249, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $3.87.\n","---\n","For customer 14549, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $4.20.\n"]}],"source":["# Define the RAG Query Function\n","def rag_chat_query(user_query: str, top_k: int = 3, llm_model: str = 'models/gemini-2.5-flash'):\n","    # 1. Retrieval: Convert query to embedding and search the vector store\n","\n","    # Generate the embedding for the user's query\n","    query_embedding_result = genai.embed_content(\n","        model=EMBEDDING_MODEL,\n","        content=[user_query],\n","        task_type=\"RETRIEVAL_QUERY\" # Optimizes query for retrieval\n","    )\n","    query_embedding = query_embedding_result['embedding'][0]\n","\n","    # Use the vector store to search for similar documents (context)\n","    retrieved_results = collection.query(\n","        query_embeddings=[query_embedding],\n","        n_results=top_k,\n","        include=['documents']\n","    )\n","\n","    # Combine retrieved documents into a single context string\n","    retrieved_context = \"\\n---\\n\".join(retrieved_results['documents'][0])\n","\n","    # 2. Augmentation \u0026 Generation: Build the prompt and call the LLM\n","\n","    # Define a system instruction for the LLM\n","    system_instruction = (\n","        \"You are an expert E-commerce Customer Retention and Analytics Chatbot. \"\n","        \"Your task is to answer user questions strictly based on the provided CONTEXT. \"\n","        \"Do not use external knowledge. Be concise and professional.\"\n","    )\n","\n","    # Create the final prompt with the retrieved context\n","    prompt = f\"\"\"\n","    CONTEXT:\n","    ---\n","    {retrieved_context}\n","    ---\n","\n","    QUESTION: {user_query}\n","\n","    ANSWER:\n","    \"\"\"\n","\n","    # Call the Gemini LLM to generate the final response\n","    response = genai.GenerativeModel(\n","        model_name=llm_model,\n","        system_instruction=system_instruction\n","    ).generate_content(prompt)\n","\n","    return response.text, retrieved_context\n","\n","# Example Query\n","user_question = \"What is the recommended retention action and projected ROI for customer 12349, and what is their recency?\"\n","answer, context = rag_chat_query(user_question, top_k=5)\n","\n","print(f\"User Question: {user_question}\")\n","print(f\"\\n--- Chatbot Answer ---\\n{answer}\")\n","print(f\"\\n--- Context Used for Answer ---\\n{context}\")"]},{"cell_type":"markdown","metadata":{"id":"wzKZnSN3otTK"},"source":["**Merging Customer Profiles with Contextual Policy Recommendations**"]},{"cell_type":"markdown","metadata":{"id":"XRTKsEqQou23"},"source":["This code snippet's primary function is to integrate two separate customer datasetsâ€”one containing Recency, Frequency, and Monetary (RFM) metrics (rfm) and the other containing contextual policy recommendations (context_recon)â€”into a single comprehensive DataFrame. It first attempts to load both dataframes, then renames the customer identifier column in the recommendation data (Customer_ID to Customer ID) for consistency. It then performs a left merge using 'Customer ID' as the key, effectively linking each customer's purchasing behavior (RFM) with their assigned retention action and projected return. Finally, it previews the new combined structure and saves the resulting dataset as merged_customer_profiles.csv."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":533,"status":"ok","timestamp":1765226641562,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"56mO2cXSmN0D","outputId":"fbddbf94-09de-4af4-d44c-dba7dc987ed2"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Merged Data Head ---\n","   Customer ID     LastPurchaseDate  Recency_x  Frequency_x  Monetary_x  \\\n","0        12346  2010-10-04 16:33:00         66           14      372.86   \n","1        12347  2010-12-07 14:57:00          2            2     1323.32   \n","2        12348  2010-09-27 14:59:00         73            1      222.16   \n","3        12349  2010-10-28 08:23:00         42            3     2064.39   \n","4        12351  2010-11-29 15:23:00         10            1      300.93   \n","\n","   Churn_Label Risk_Tier         Country  Recency_y  Frequency_y  Monetary_y  \\\n","0            1    Medium  United Kingdom         66           14      372.86   \n","1            0       NaN         Iceland          2            2     1323.32   \n","2            1       NaN         Finland         73            1      222.16   \n","3            1    Medium           Italy         42            3     2064.39   \n","4            0       NaN     Unspecified         10            1      300.93   \n","\n","  Chosen_Action  Chosen_Score  Estimated_Reward  \n","0   call+coupon    231.325229          2.152547  \n","1         email    793.994494          0.000000  \n","2          none    140.312878          0.000000  \n","3           sms   1358.440668          2.989122  \n","4    sms+coupon    180.661649          0.000000  \n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","# Re-load dataframes to ensure they are available\n","try:\n","    rfm = pd.read_csv(\"rfm.xls\")\n","    context_recon = pd.read_csv(\"contextual_policy_recommendations.xls\")\n","except FileNotFoundError as e:\n","    print(f\"Error loading files. Ensure 'rfm.csv' and 'contextual_policy_recommendations.csv' are in the directory: {e}\")\n","    # Handle error or exit\n","\n","# Rename the key column in context_recon to match rfm before merging\n","context_recon = context_recon.rename(columns={'Customer_ID': 'Customer ID'})\n","\n","# Merge the two dataframes on 'Customer ID'\n","merged_customer_data = pd.merge(rfm, context_recon, on='Customer ID', how='left')\n","\n","# Preview the new merged data structure\n","print(\"--- Merged Data Head ---\")\n","print(merged_customer_data.head())\n","\n","# Save the merged DataFrame for inspection (optional)\n","merged_customer_data.to_csv(\"merged_customer_profiles.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"VwcSzTSdo8MS"},"source":["**Generating Comprehensive Customer Profile Documents**"]},{"cell_type":"markdown","metadata":{"id":"oCnF1u22il0c"},"source":["This code snippet's purpose is to create comprehensive, single-source documents for each customer by combining their behavioral data with their strategic recommendations. It iterates through the merged_customer_data DataFrame, which contains both RFM metrics (Recency, Frequency, Monetary) and policy recommendations (Chosen Action, Estimated Reward). For every row, it extracts all these distinct features and synthesizes them into one rich, natural-language string. These detailed profile strings are collected into the merged_docs list, which are highly informative documents ready for use in advanced downstream systems like a Retrieval-Augmented Generation (RAG) knowledge base."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1765226641844,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"pbZJWQGsmNwG","outputId":"7bf15c7e-de8f-4a8e-ebdf-4a746a4294ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Example of Improved Document for Customer 12349 ---\n","Customer Profile 12349: Recency is 42 days, Frequency is 3 times, and Monetary spend is $2064.39. This customer is classified as churned. The recommended retention action is to use 'sms' with a projected ROI of $2.99.\n"]}],"source":["# Create new documents from the merged DataFrame\n","merged_docs = []\n","for index, row in merged_customer_data.iterrows():\n","    customer_id = row['Customer ID']\n","    recency = row['Recency_x']\n","    frequency = row['Frequency_x']\n","    monetary = row['Monetary_x']\n","    churn = 'churned' if row['Churn_Label'] == 1 else 'not churned'\n","    action = row['Chosen_Action']\n","    reward = row['Estimated_Reward']\n","\n","    # Combine all information into one single, rich document\n","    text = (\n","        f\"Customer Profile {customer_id}: \"\n","        f\"Recency is {recency} days, Frequency is {frequency} times, \"\n","        f\"and Monetary spend is ${monetary:.2f}. \"\n","        f\"This customer is classified as {churn}. \"\n","        f\"The recommended retention action is to use '{action}' \"\n","        f\"with a projected ROI of ${reward:.2f}.\"\n","    )\n","    merged_docs.append(text)\n","\n","# Example of the improved document for customer 12349\n","# (The index for 12349 is 3 based on the printout from the first notebook cell)\n","print(\"\\n--- Example of Improved Document for Customer 12349 ---\")\n","print(merged_docs[3])"]},{"cell_type":"markdown","metadata":{"id":"4jFhQBF0tUI9"},"source":["**Initiating and Running the Interactive RAG Chatbot Session**"]},{"cell_type":"markdown","metadata":{"id":"3OVpJCkrtY5v"},"source":["This code snippet defines the function start_chatbot_session(), which establishes and runs a continuous, interactive conversational loop for the E-commerce Retrieval-Augmented Generation (RAG) system. The function initializes the user interface, prompting the user for input. Inside a while loop, it accepts a user query, checks for exit commands (quit or exit), and then passes the input to the previously defined rag_chat_query function. This query function retrieves relevant context from the vector database and uses the Gemini LLM to generate an answer. The loop continuously prints the chatbot's response and, for transparency, the source context used to generate that answer, until the user explicitly terminates the session."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1765226641859,"user":{"displayName":"N M Emran Hussain","userId":"03540980603548680585"},"user_tz":300},"id":"wpQaa5oWmNpq"},"outputs":[],"source":["# Define and Run the Continuous Chat Loop\n","import os\n","import google.generativeai as genai\n","\n","# NOTE: The rag_chat_query function and the 'collection' object are assumed\n","# to be defined and configured from the previous successful steps.\n","# The EMBEDDING_MODEL is 'models/gemini-embedding-001' and LLM is 'models/gemini-2.5-flash'.\n","\n","def start_chatbot_session():\n","    \"\"\"Initializes and runs the continuous RAG chatbot session.\"\"\"\n","\n","    print(\"--- E-commerce RAG Chatbot Initialized ---\")\n","    print(\"Ask a question about customer data or retention policies.\")\n","    print(\"Type 'quit' or 'exit' to end the session.\\n\")\n","\n","    # Main conversational loop\n","    while True:\n","        # Get user input\n","        user_input = input(\"You: \")\n","\n","        # Check for exit commands\n","        if user_input.lower() in [\"quit\", \"exit\"]:\n","            print(\"\\nChatbot session ended. Goodbye!\")\n","            break\n","\n","        if not user_input.strip():\n","            continue\n","\n","        try:\n","            # Call the RAG function (from the previous step)\n","            # We use top_k=1 since the customer data is now fully merged\n","            # For general policy questions, top_k can be higher (e.g., 3)\n","            answer, context = rag_chat_query(user_input, top_k=3)\n","\n","            print(f\"\\nğŸ¤– Chatbot: {answer}\")\n","\n","            # Optionally, show the source context for verification\n","            print(\"\\n[Source Context Retrieved]:\")\n","            print(context)\n","            print(\"-------------------------------------------\\n\")\n","\n","        except Exception as e:\n","            print(f\"\\nâŒ An error occurred: {e}. Please try again.\")\n"]},{"cell_type":"markdown","metadata":{"id":"2TNa7DI593Y4"},"source":["**Interactive DashBoard**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3eaF-21cAoce"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- E-commerce RAG Chatbot Initialized ---\n","Ask a question about customer data or retention policies.\n","Type 'quit' or 'exit' to end the session.\n","\n"]}],"source":["# Start the Chatbot\n","start_chatbot_session()"]},{"cell_type":"markdown","metadata":{"id":"G162lUXd_WkG"},"source":["Python code for 'dashboard_app.py' (do not run here)"]},{"cell_type":"markdown","metadata":{"id":"oFvl_MVv_r4y"},"source":["Open a .txt file in Desktop and save the code of 'dashboard_app.py' and name the .txt fille accordingly as 'dashboard_app.py'"]},{"cell_type":"markdown","metadata":{"id":"g9B2-bK2-G3w"},"source":["**We have to use Powershell 7 for command/prompt**\n","\n","1. copy this part 'pip install streamlit pandas numpy plotly' and press enter\n","2. type 'cd OneDrive' and press enter\n","3. type 'cd Desktop' and press enter\n","4. type 'streamlit run dashboard_app.py' and see the magic\n"]},{"cell_type":"markdown","metadata":{"id":"3gRJXMkAKp2w"},"source":["1. $env:GOOGLE_API_KEY = \"your-real-key\"\n","2. python prepare_vector_store.py ...\n","3. streamlit run streamlit_app.py\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPPm0yDd6yFsgvwWchwscIZ","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}